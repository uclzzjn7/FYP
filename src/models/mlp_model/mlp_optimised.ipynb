{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UCL COMP0029 Individual Project for Year 3 BSc\n",
    "### Robust Robotic Grasping Utilising Touch Sensing - Proposed Learning Framework Notebook\n",
    "This notebook contains the essential code for the proposed Multilayer Perceptron (MLP) approach to grasp stability prediction. The whole notebook should take approximately 10 minutes to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set device for `PyTorch` training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.set_num_threads(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.errstate(invalid='ignore', divide='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load datasets from saved .npy files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To collect data for this experiment, you can run the \"Collect Sensory Data\" button in the Pybullet simulation. This generates a predefined number of Gaussian grasps randomly generated from a base hand pose. Each individual grasp is considered as an individual experiment, and the data collected from this experiment is split into four, each stored in its own dataset.\n",
    "\n",
    "For all object models used in this experiment, each object has 4 datasets which include:\n",
    "- `depth_ds.npy` which stores the depth tactile data from the mounted DIGIT sensors\n",
    "- `color_ds.npy` which stores the colored (RGB) version of the depth tactile data from the mounted DIGIT sensors\n",
    "- `poses_ds.npy` which stores the randomly-generated 6d hand poses from the simulation\n",
    "- `outcomes_ds.npy` which stores the outcomes of each random pose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will use the following dataset combinations:\n",
    "- `tactile` only (depth + colour)\n",
    "- `visual` without geometric features\n",
    "- `visual` with geometric features\n",
    "- `multi-modal` consisting of `tactile` and `visual`\n",
    "- `complete` consisting of `tactile` and `visual` with geometric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"../../datasets/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_names = [\"block1\", \"block2\", \"block3\", \"cylinder1\", \"cylinder2\", \"cylinder3\", \"bottle1\", \"bottle2\", \"bottle3\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load `.npy` datasets via `np.load()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_data = np.empty((0, 2, 160, 120))\n",
    "color_data = np.empty((0, 2, 160, 120, 3))\n",
    "poses_data = np.empty((0, 6))\n",
    "grasp_outcomes_data = np.empty((0,))\n",
    "\n",
    "for object_name in object_names:\n",
    "    # Construct the relative paths of each dataset and load them into the notebook\n",
    "    depth_data_temp = np.load(root + object_name + \"_ds/depth_ds.npy\", mmap_mode='r')\n",
    "    colour_data_temp = np.load(root + object_name + \"_ds/color_ds.npy\", mmap_mode='r')\n",
    "    poses_data_temp = np.load(root + object_name + \"_ds/poses_ds.npy\", mmap_mode='r')\n",
    "    grasp_labels_temp = np.load(root + object_name + \"_ds/grasp_outcomes.npy\", mmap_mode='r')\n",
    "\n",
    "    depth_data = np.append(depth_data, depth_data_temp, axis=0)\n",
    "    color_data = np.append(color_data, colour_data_temp, axis=0)\n",
    "    poses_data = np.append(poses_data, poses_data_temp, axis=0)\n",
    "    grasp_outcomes_data = np.append(grasp_outcomes_data, grasp_labels_temp, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del depth_data_temp, colour_data_temp, poses_data_temp, grasp_labels_temp\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These datasets should all be in the form of $(N\\times...)$ where $N$ is the number of examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of depth_data: {depth_data.shape}\")\n",
    "print(f\"Shape of color_data: {color_data.shape}\")\n",
    "print(f\"Shape of poses_data: {poses_data.shape}\")\n",
    "print(f\"Shape of grasp_outcomes_data: {grasp_outcomes_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we confirm the number of successful and unsuccessful grasps recorded. This helps us in the next section to determine how many examples we should include for each class in order to produce a balanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"# of sucessesful grasps: {(grasp_outcomes_data == 1).sum()}\")\n",
    "print(f\"# of unsuccessful grasps: {(grasp_outcomes_data == 0).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we create the required datasets specified from section 2, and normalise the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(arr):\n",
    "    # Normalize & standardize each column\n",
    "    mean = np.mean(arr, axis=0)\n",
    "    std = np.std(arr, axis=0)\n",
    "    \n",
    "    arr = (arr - mean) / std\n",
    "    arr = np.nan_to_num(arr, 0)\n",
    "    arr[np.isinf(arr)] = 0\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depth and colour data consists of pairs of tactile sensory readings. Thus, we concatenate each pair together into single images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_data = np.concatenate((depth_data[:, 0], depth_data[:, 1]), axis=2)\n",
    "color_data = np.concatenate((color_data[:, 0], color_data[:, 1]), axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_data = torch.from_numpy(normalize(depth_data))\n",
    "color_data = torch.from_numpy(normalize(color_data))\n",
    "visual_data = torch.from_numpy(np.nan_to_num(normalize(poses_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of depth_ds: {depth_data.shape}\")\n",
    "print(f\"Shape of colour_ds: {color_data.shape}\")\n",
    "print(f\"Shape of visual_ds: {visual_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the tactile dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tactile_data = torch.cat([depth_data.unsqueeze(-1), color_data], dim=-1)\n",
    "tactile_data = torch.nan_to_num(tactile_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of tactile_ds: {tactile_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del depth_data, color_data, poses_data\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple convolutional neural network that extracts features from an input tensor\n",
    "class FeatureExtractorCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractorCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=3)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data using CNN feature extraction\n",
    "cnn = FeatureExtractorCNN()\n",
    "cnn_tactile = torch.cat([cnn(img.float().permute(2,0,1)).unsqueeze(0) for img in tactile_data])\n",
    "cnn_tactile = cnn_tactile.reshape(cnn_tactile.shape[0], -1)\n",
    "cnn_tactile.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tactile_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Testing different data representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our MLP model, there are two fully connected (dense) layers, each with an activation function (ReLU for the first layer and no activation for the second layer). The input size, hidden size, and output size are parameters that need to be specified when creating an instance of the MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        self.optimizer = optim.Adam(self.parameters())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "        \n",
    "    def train_mlp(self, epochs, X_train, y_train):\n",
    "        self.losses = []\n",
    "        for i in range(epochs):\n",
    "            inputs = torch.from_numpy(X_train).float()\n",
    "            labels = torch.from_numpy(y_train).float().view(-1, 1)\n",
    "\n",
    "            # Zero the gradients\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            preds = self(inputs)\n",
    "            loss = self.criterion(preds, labels)\n",
    "            self.losses.append(loss.item())\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    def eval_mlp(self, X_test, y_test, verbose=False):\n",
    "        # Evaluate the performance of the model on the testing set\n",
    "        with torch.no_grad():\n",
    "            inputs = torch.from_numpy(X_test).float()\n",
    "            labels = torch.from_numpy(y_test).float().view(-1, 1)\n",
    "\n",
    "            # Forward pass\n",
    "            final_preds = self(inputs)\n",
    "            predicted = (final_preds > 0).float()\n",
    "            \n",
    "            # Confusion matrix\n",
    "            if verbose:\n",
    "                cm = confusion_matrix(y_test, predicted)\n",
    "                sns.heatmap(cm, linewidths=1, annot=True, fmt='g')\n",
    "\n",
    "            # Performance metrics\n",
    "            accuracy = round(accuracy_score(labels, predicted) * 100, 2)\n",
    "            precision = round(precision_score(labels, predicted) * 100, 2)\n",
    "            recall = round(recall_score(labels, predicted) * 100, 2)\n",
    "            f1 = round(f1_score(labels, predicted) * 100, 2)\n",
    "            return accuracy, precision, recall, f1\n",
    "    \n",
    "    def plot_losses(self):\n",
    "        plt.plot(self.losses)\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Training Loss')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4a. Testing datatset combinations (CNN dimensionality-reduced) without geometric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mlp_multitrial(dataset, grasp_outcomes_data, trials_count: int = 5, sampling=False):\n",
    "    performance_metrics = np.empty((0, 4))\n",
    "    dataset = dataset.detach().numpy() if isinstance(dataset, torch.Tensor) else dataset\n",
    "\n",
    "    for i in range(trials_count):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(dataset, grasp_outcomes_data, test_size=0.2)\n",
    "        mlp = MLP(input_size=X_train.shape[1], hidden_size=64, output_size=1)\n",
    "        mlp.train_mlp(epochs=500, X_train=X_train, y_train=y_train)\n",
    "        accuracy, precision, recall, f1 = mlp.eval_mlp(X_test=X_test, y_test=y_test)\n",
    "        metrics_row = np.array([accuracy, precision, recall, f1]).reshape(1, 4)\n",
    "        performance_metrics = np.append(performance_metrics, metrics_row, axis=0)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "    return performance_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Tactile-only dataset + CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = train_mlp_multitrial(cnn_tactile, grasp_outcomes_data, trials_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tactile-only MLP\")\n",
    "print(f\"Accuracy:{metrics[:, 0]}\")\n",
    "print(f\"Mean accuracy score: {np.mean(metrics[:, 0]):.2f} + {np.std(metrics[:, 0]):.2f}\")\n",
    "print(f\"Mean precision score: {np.mean(metrics[:, 1]):.2f}  + {np.std(metrics[:, 1]):.2f}\")\n",
    "print(f\"Mean recall score: {np.mean(metrics[:, 2]):.2f}  + {np.std(metrics[:, 2]):.2f}\")\n",
    "print(f\"Mean f1 score: {np.mean(metrics[:, 3]):.2f} + {np.std(metrics[:, 3]):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Visual-only dataset + CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = train_mlp_multitrial(visual_data, grasp_outcomes_data, trials_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Visual-only MLP\")\n",
    "print(f\"Accuracy:{metrics[:, 0]}\")\n",
    "print(f\"Mean accuracy score: {np.mean(metrics[:, 0]):.2f} + {np.std(metrics[:, 0]):.2f}\")\n",
    "print(f\"Mean precision score: {np.mean(metrics[:, 1]):.2f}  + {np.std(metrics[:, 1]):.2f}\")\n",
    "print(f\"Mean recall score: {np.mean(metrics[:, 2]):.2f}  + {np.std(metrics[:, 2]):.2f}\")\n",
    "print(f\"Mean f1 score: {np.mean(metrics[:, 3]):.2f} + {np.std(metrics[:, 3]):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Multi-modal dataset (tactile + visual) + CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We simply combine the cnn-processed tactile data (from Section 5.3.1) with the visual data\n",
    "cnn_complete = torch.cat([cnn_tactile.reshape(cnn_tactile.shape[0], -1), visual_data], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = train_mlp_multitrial(cnn_complete, grasp_outcomes_data, trials_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Multi-modal MLP\")\n",
    "print(f\"Accuracy:{metrics[:, 0]}\")\n",
    "print(f\"Mean accuracy score: {np.mean(metrics[:, 0]):.2f} + {np.std(metrics[:, 0]):.2f}\")\n",
    "print(f\"Mean precision score: {np.mean(metrics[:, 1]):.2f}  + {np.std(metrics[:, 1]):.2f}\")\n",
    "print(f\"Mean recall score: {np.mean(metrics[:, 2]):.2f}  + {np.std(metrics[:, 2]):.2f}\")\n",
    "print(f\"Mean f1 score: {np.mean(metrics[:, 3]):.2f} + {np.std(metrics[:, 3]):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del cnn_complete\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Determining impact of geometric features of objects on MLP accuracy\n",
    "This section aims to determine the capability of the selected object features (principal components) in identifying primitive object classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=2, formatter={'float_kind': \"{:.3f}\".format})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5a. Clustering for object features using PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading object features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Object features\n",
    "block_features = np.array([\n",
    "    [0.025, 0.05, 0.05, 0., 0., 0., 0., 0., 0.],\n",
    "    [0.03, 0.025, 0.045, 0., 0., 0., 0., 0., 0.],\n",
    "    [0.05, 0.025, 0.04, 0., 0., 0., 0., 0., 0.],\n",
    "])\n",
    "\n",
    "cylinder_features = np.array([\n",
    "    [0.04, 0.04, 0.05, 950.21606561, 14540.28434464, 950.21606561, 14540.28434464, 950.21606561, 14540.28434464],\n",
    "    [0.045, 0.045, 0.035, 750.78800246, 11488.6197291, 750.78800246, 11488.6197291, 750.78800246, 11488.6197291],\n",
    "    [0.034, 0.034, 0.045, 1315.17794549, 20124.96103064, 1315.17794549, 20124.96103064, 1315.17794549, 20124.96103064]\n",
    "])\n",
    "\n",
    "bottle_features = np.array([\n",
    "    [0.06, 0.04, 0.04, 43195.64459266, 114198.0441697 , 45229.93706864, 135651.61794731, 75768.06626518,  83802.00991944],\n",
    "    [0.04, 0.06, 0.06, 68993.34322089, 220902.86884084, 75354.47923164, 239160.99530455, 109695.41304924, 147938.06047763],\n",
    "    [0.04, 0.06, 0.04, 61744.75459905, 168925.8805044 , 68143.84372052, 148775.07141178, 73102.67367022, 102953.40831915]\n",
    "])\n",
    "\n",
    "object_names = {0: 'cylinder', 1: 'block', 2: 'bottle'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature transformation - concatenating features together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_features = (block_features, cylinder_features, bottle_features)\n",
    "features = np.concatenate(object_features)\n",
    "labels = np.array([0, 0, 0, 1, 1, 1, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "features = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(data, k, verbose=True):\n",
    "    pca = PCA(n_components=k)\n",
    "    pca.fit(data)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Variance captured: {sum(pca.explained_variance_ratio_)*100:.2f}%\")\n",
    "    return pca.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the number of components to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 5):\n",
    "    pca_res = pca(features, k=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot 3-feature PCA results\n",
    "pca3d = pca(features, k=3)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "colors = ['r', 'g', 'b']\n",
    "for i, c in zip(np.unique(labels), colors):\n",
    "    ax.scatter(pca3d[labels == i, 0], pca3d[labels == i, 1], pca3d[labels == i, 2], c=c, label=object_names[i])\n",
    "\n",
    "ax.set_xlabel('Component 1')\n",
    "ax.set_ylabel('Component 2')\n",
    "ax.set_zlabel('Component 3')\n",
    "ax.set_aspect('equal', 'box')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5b. MLP: Dataset combination 4: Visual (EE pose and geometric features) + CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = np.repeat(np.concatenate(object_features), 160, axis=0)\n",
    "\n",
    "visual_data = np.concatenate((visual_data, blocks), axis=1)\n",
    "visual_data = torch.from_numpy(np.nan_to_num(normalize(visual_data)))\n",
    "visual_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del object_features, blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Visual-only (EE pose + geometric features) + CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = train_mlp_multitrial(visual_data, grasp_outcomes_data, trials_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Visual data (EE pose + geo) MLP\")\n",
    "print(f\"Accuracy:{metrics[:, 0]}\")\n",
    "print(f\"Mean accuracy score: {np.mean(metrics[:, 0]):.2f} + {np.std(metrics[:, 0]):.2f}\")\n",
    "print(f\"Mean precision score: {np.mean(metrics[:, 1]):.2f}  + {np.std(metrics[:, 1]):.2f}\")\n",
    "print(f\"Mean recall score: {np.mean(metrics[:, 2]):.2f}  + {np.std(metrics[:, 2]):.2f}\")\n",
    "print(f\"Mean f1 score: {np.mean(metrics[:, 3]):.2f} + {np.std(metrics[:, 3]):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5c. MLP: Dataset combination 5: Tactile + Visual (EE pose and geometric features) + CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset = torch.cat((cnn_tactile, visual_data), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = train_mlp_multitrial(final_dataset, grasp_outcomes_data, trials_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Final dataset (no PCA) MLP\")\n",
    "print(f\"Accuracy:{metrics[:, 0]}\")\n",
    "print(f\"Mean accuracy score: {np.mean(metrics[:, 0]):.2f} + {np.std(metrics[:, 0]):.2f}\")\n",
    "print(f\"Mean precision score: {np.mean(metrics[:, 1]):.2f}  + {np.std(metrics[:, 1]):.2f}\")\n",
    "print(f\"Mean recall score: {np.mean(metrics[:, 2]):.2f}  + {np.std(metrics[:, 2]):.2f}\")\n",
    "print(f\"Mean f1 score: {np.mean(metrics[:, 3]):.2f} + {np.std(metrics[:, 3]):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del cnn_tactile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5d. MLP: Selecting number of components for PCA (with dataset combination 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We conduct the following experiment to determine the optimal value of $k$ for PCA:\n",
    "1. Train the MLP from $k=1$ to $k=20$\n",
    "2. Repeat 1 for 5 times to find the average accuracy for each $k$, and choose $k$ value for the highest mean accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_K, MAX_K = 1, 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_accuracies_mean = []\n",
    "pca_accuracies_std = []\n",
    "for i in range(MIN_K, MAX_K+1):\n",
    "    transformed_dataset = pca(final_dataset.detach().numpy(), k=i, verbose=False)\n",
    "    transformed_dataset = torch.from_numpy(transformed_dataset)\n",
    "    metrics = train_mlp_multitrial(transformed_dataset, grasp_outcomes_data)\n",
    "    pca_accuracies_mean.append(np.mean(metrics[:, 0]))\n",
    "    pca_accuracies_std.append(np.std(metrics[:, 0]))\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"Completed PCA training for k={i} components\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del transformed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del visual_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the average accuracies for each $k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_accuracies_mean = np.array(pca_accuracies_mean)\n",
    "pca_accuracies_std = np.array(pca_accuracies_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = pca_accuracies_mean.argmax(axis=0)\n",
    "print(f\"Number of components for PCA: {K}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(MIN_K, MAX_K+1)\n",
    "plt.errorbar(x, pca_accuracies_mean, yerr=pca_accuracies_std, fmt='o', markersize=5, ecolor='red', capsize=3, capthick=1)\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"Number of principal components\")\n",
    "plt.xticks(np.arange(MIN_K, MAX_K+1, step=2))\n",
    "plt.ylabel(\"Mean MLP accuracy (%)\")\n",
    "plt.title(\"Impact of Number of Principal Components on MLP Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(MIN_K, MAX_K+1, step=1)\n",
    "plt.plot(x, pca_accuracies_mean)\n",
    "plt.fill_between(x, pca_accuracies_mean-pca_accuracies_std, pca_accuracies_mean+pca_accuracies_std, alpha=0.2)\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"Number of principal components\")\n",
    "plt.xticks(np.arange(MIN_K, MAX_K+1, step=2))\n",
    "plt.ylabel(\"Mean MLP accuracy (%)\")\n",
    "plt.title(\"Impact of Number of Principal Components on MLP Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_accuracy_indices = np.argsort(pca_accuracies_mean)[-5:] \n",
    "top_accuracies = pca_accuracies_mean[top_accuracy_indices]\n",
    "top_stds = pca_accuracies_std[top_accuracy_indices]\n",
    "\n",
    "print(\"Top 5 mean accuracies:\", top_accuracies)\n",
    "print(\"Top 5 std accuracies:\", top_stds)\n",
    "print(\"Corresponding indices:\", top_accuracy_indices + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We update the train_mlp_multitrial function to accomodate pre-defined training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mlp_multitrial2(X_train, X_test, y_train, y_test, trials_count: int = 5):\n",
    "    performance_metrics = np.empty((0, 4))\n",
    "\n",
    "    for i in range(trials_count):\n",
    "        mlp = MLP(input_size=X_train.shape[1], hidden_size=64, output_size=1)\n",
    "        mlp.train_mlp(epochs=500, X_train=X_train, y_train=y_train)\n",
    "        accuracy, precision, recall, f1 = mlp.eval_mlp(X_test=X_test, y_test=y_test)\n",
    "        metrics_row = np.array([accuracy, precision, recall, f1]).reshape(1, 4)\n",
    "        performance_metrics = np.append(performance_metrics, metrics_row, axis=0)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return performance_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Train MLP on 2 blocks, test on remaining block\n",
    "Objective: see MLP's robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = [\n",
    "    [[1, 2], [3]],\n",
    "    [[1, 3], [2]],\n",
    "    [[2, 3], [1]]\n",
    "]\n",
    "\n",
    "exp1_accuracies_mean = []\n",
    "exp1_accuracies_std = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset = final_dataset.detach().numpy()\n",
    "\n",
    "for i in range(3):  # For each primitive object type (box, cylinder, bottle)\n",
    "    stidx = 480 * i # Starting index to slice final dataset\n",
    "    for combination in combinations:\n",
    "        training, testing = combination[0], combination[1]\n",
    "        X_train1 = final_dataset[(stidx + (training[0] - 1) * 160 + 1):(stidx + (training[0] * 160))]\n",
    "        X_train2 = final_dataset[(stidx + (training[1] - 1) * 160 + 1):(stidx + (training[1] * 160))]\n",
    "        X_train = np.vstack((X_train1, X_train2))\n",
    "\n",
    "        y_train1 = grasp_outcomes_data[(stidx + (training[0] - 1) * 160 + 1):(stidx + (training[0] * 160))]\n",
    "        y_train2 = grasp_outcomes_data[(stidx + (training[1] - 1) * 160 + 1):(stidx + (training[1] * 160))]\n",
    "        y_train = np.concatenate((y_train1, y_train2))\n",
    "        \n",
    "\n",
    "        X_test = final_dataset[(stidx + (testing[0] - 1) * 160 + 1):(stidx + (testing[0] * 160))]\n",
    "        y_test = grasp_outcomes_data[stidx + (testing[0] - 1) * 160 + 1:stidx + (testing[0] * 160)]\n",
    "        metrics = train_mlp_multitrial2(X_train, X_test, y_train, y_test)\n",
    "        exp1_accuracies_mean.append(np.mean(metrics[:, 0]))\n",
    "        exp1_accuracies_std.append(np.std(metrics[:, 0]))\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train, X_test, y_train, y_test, training, testing, combination, combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1_accuracies_mean = np.array(exp1_accuracies_mean)\n",
    "exp1_accuracies_std = np.array(exp1_accuracies_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1_accuracies_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1_accuracies_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Train MLP on randomly sampled subset of all objects\n",
    "Objective: see influence of dataset size on accuracy;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly sample a dataset of the specified size without shuffling the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consistent_sampling(dataset1, dataset2, input_size):\n",
    "    num_rows = len(dataset1)\n",
    "    indices = np.random.choice(len(dataset1), input_size, replace=False)\n",
    "    samples1, samples2 = [], []\n",
    "    for i in indices:\n",
    "        samples1.append(dataset1[i])\n",
    "        samples2.append(dataset2[i])\n",
    "    \n",
    "    samples1 = np.stack(samples1, axis=0)\n",
    "    samples2 = np.stack(samples2, axis=0)\n",
    "    return samples1, samples2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp2_accuracies_mean = []\n",
    "exp2_accuracies_std = []\n",
    "\n",
    "for input_size in range(100, 1500, 100):\n",
    "    dataset, grasp_labels = consistent_sampling(final_dataset, grasp_outcomes_data, input_size=input_size)\n",
    "    metrics = train_mlp_multitrial(dataset, grasp_labels)\n",
    "    exp2_accuracies_mean.append(np.mean(metrics[:, 0]))\n",
    "    exp2_accuracies_std.append(np.std(metrics[:, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp2_accuracies_mean = np.array(exp2_accuracies_mean)\n",
    "exp2_accuracies_std = np.array(exp2_accuracies_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(100, 1500, step=100)\n",
    "plt.plot(x, exp2_accuracies_mean)\n",
    "plt.fill_between(x, exp2_accuracies_mean-exp2_accuracies_std, exp2_accuracies_mean+exp2_accuracies_std, alpha=0.2)\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"Sample size\")\n",
    "plt.xticks(np.arange(100, 1500, step=100), rotation=45)\n",
    "plt.ylabel(\"Mean MLP Accuracy (%)\")\n",
    "plt.title(\"Impact of Sample Size on MLP Performance\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "ee1153b3f7d4e5dcd3632df88b5826f13fc1401ea1edd930ba216b62830b02e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
