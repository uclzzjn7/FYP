{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UCL COMP0029 Individual Project for Year 3 BSc\n",
    "### Robust Robotic Grasping Utilising Touch Sensing - Baseline Approach Notebook\n",
    "This notebook contains the code for developing a baseline approach to grasping using classifiers: given some combinations of tactile data, end effector poses relative to the robot hand (visual data), etc., determine whether these constraints will produce a successful/unsuccessful grasp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import gc\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from math import sin, cos\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set device for `PyTorch` training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empty PyTorch cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set numpy seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load datasets from saved .npy files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To collect data for this experiment, you can run the \"Collect Sensory Data\" button in the Pybullet simulation. This generates a predefined number of Gaussian grasps randomly generated from a base hand pose. Each individual grasp is considered as an individual experiment, and the data collected from this experiment is split into four, each stored in its own dataset.\n",
    "\n",
    "For all object models used in this experiment, each object has 4 datasets which include:\n",
    "- `depth_ds.npy` which stores the depth tactile data from the mounted DIGIT sensors\n",
    "- `color_ds.npy` which stores the colored (RGB) version of the depth tactile data from the mounted DIGIT sensors\n",
    "- `poses_ds.npy` which stores the randomly-generated 6d hand poses from the simulation\n",
    "- `outcomes_ds.npy` which stores the outcomes of each random pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"../../datasets/\"\n",
    "object_name = \"block\"      # object_name should be in [bottle, block, mug]\n",
    "\n",
    "depth_data = np.load(root + object_name + \"/depth_ds.npy\")\n",
    "color_data = np.load(root + object_name + \"/color_ds.npy\")\n",
    "poses_data = np.load(root + object_name + \"/poses_ds.npy\")\n",
    "grasp_outcomes_data = np.load(root + object_name + \"/grasp_outcomes.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These datasets should all be in the form of $(N\\times...)$ where $N$ is the number of examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of depth_data: {depth_data.shape}\")\n",
    "print(f\"Shape of color_data: {color_data.shape}\")\n",
    "print(f\"Shape of poses_data: {poses_data.shape}\")\n",
    "print(f\"Shape of grasp_outcomes_data: {grasp_outcomes_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we confirm the number of successful and unsuccessful grasps recorded. This helps us in the next section to determine how many examples we should include for each class in order to produce a balanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"# of sucessesful grasps: {(grasp_outcomes_data == 1).sum()}\")\n",
    "print(f\"# of unsuccessful grasps: {(grasp_outcomes_data == 0).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train a logistic regression classifier on 3 combinations of data:\n",
    "- **Tactile** only: concatenated and flattened depth and color data\n",
    "- **Visual** only: 6D end effector poses consisting of position (x,y,z) and orientation (r,p,y) data\n",
    "- **Both**: concatenated and flattened tactile and visual data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define some reusable functions for later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(arr):\n",
    "    # Normalize & standardize each column\n",
    "    mean = np.mean(arr, axis=0)\n",
    "    std = np.std(arr, axis=0)\n",
    "    \n",
    "    arr = (arr - mean) / std\n",
    "    arr = np.nan_to_num(arr, 0)\n",
    "    arr[np.isinf(arr)] = 0\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since each tactile reading (depth and color) is a pair of images (one on each finger), we concatenate them together as a single 160x240 image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_data = np.concatenate((depth_data[:, 0], depth_data[:, 1]), axis=2)\n",
    "color_data = np.concatenate((color_data[:, 0], color_data[:, 1]), axis=2)\n",
    "print(f\"Shape of depth_data: {depth_data.shape}\")\n",
    "print(f\"Shape of color_data: {color_data.shape}\")\n",
    "\n",
    "depth_ds = torch.from_numpy(normalize(depth_data))\n",
    "color_ds = torch.from_numpy(normalize(color_data))\n",
    "visual_ds = torch.from_numpy(np.nan_to_num(normalize(poses_data)))\n",
    "visual_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then concatenate the depth and color datasets to produce the flattened tactile dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tactile_ds = torch.cat([depth_ds.unsqueeze(-1), color_ds], dim=-1)\n",
    "tactile_ds = torch.nan_to_num(tactile_ds)\n",
    "complete_ds = torch.cat([tactile_ds.reshape(tactile_ds.shape[0], -1), visual_ds], dim=1)\n",
    "complete_ds = torch.nan_to_num(complete_ds)\n",
    "tactile_ds.shape, complete_ds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Dataset visualisation (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only run this section to produce figures and plots for the project report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_PADDING = 0.2        # Adjust for visualisation of end poses\n",
    "\n",
    "\n",
    "def plot_ee_pose(pose_data, grasp_outcome, ax, orientation):\n",
    "    x, y, z, alpha, beta, gamma = pose_data\n",
    "    z -= Z_PADDING * 1.6\n",
    "\n",
    "    # create rotation matrix based on Euler angles\n",
    "    Rx = np.array([[1, 0, 0], [0, cos(alpha), sin(alpha)], [0, -sin(alpha), cos(alpha)]])\n",
    "    Ry = np.array([[cos(beta), 0, -sin(beta)], [0, 1, 0], [sin(beta), 0, cos(beta)]])\n",
    "    Rz = np.array([[cos(gamma), sin(gamma), 0], [-sin(gamma), cos(gamma), 0], [0, 0, 1]])\n",
    "    R = Rz.dot(Ry.dot(Rx))\n",
    "\n",
    "    # calculate endpoints of line based on orientation\n",
    "    vec = np.array([0, 0.015, 0])\n",
    "    vec_rotated1 = R.dot(vec)\n",
    "    endpoint1 = [x + vec_rotated1[0], y + vec_rotated1[1], z + vec_rotated1[2]]\n",
    "    \n",
    "    vec_rotated2 = R.dot(-vec)\n",
    "    endpoint2 = [x + vec_rotated2[0], y + vec_rotated2[1], z + vec_rotated2[2]]\n",
    "\n",
    "    # set midpoint to the actual point\n",
    "    midpoint = [x, y, z]\n",
    "\n",
    "    # plot line through point in orientation direction\n",
    "    ax.plot([endpoint1[0], endpoint2[0]], [endpoint1[1], endpoint2[1]], [endpoint1[2], endpoint2[2]], color='green' if grasp_outcome==1 else 'red', zorder=20)\n",
    "\n",
    "    # plot vertical lines starting from endpoints\n",
    "    ax.plot([endpoint1[0], endpoint1[0]], [endpoint1[1], endpoint1[1]], [endpoint1[2], endpoint1[2]-0.035], color='green' if grasp_outcome==1 else 'red', zorder=20)\n",
    "    ax.plot([endpoint2[0], endpoint2[0]], [endpoint2[1], endpoint2[1]], [endpoint2[2], endpoint2[2]-0.035], color='green' if grasp_outcome==1 else 'red', zorder=20)\n",
    "\n",
    "    # Display plot viewing orientation\n",
    "    ax.view_init(elev=orientation[0], azim=orientation[1], roll=orientation[2])\n",
    "\n",
    "\n",
    "# Draw block object\n",
    "def plot_3d_box(ax, orientation):\n",
    "    # Dimensions of the box object: (W=0.025, H=0.05, D=0.05)\n",
    "    # Dimensions for unit box: (0,0,0), (0,1,0), (1,0,0), (0,0,1)\n",
    "    cube_definition = [(-0.0125, -0.025, 0), (0.0125, -0.025, 0), (-0.0125, 0.0255, 0), (-0.0125, -0.025, 0.05)]\n",
    "    cube_definition_array = [np.array(list(item)) for item in cube_definition]\n",
    "    points = []\n",
    "    points += cube_definition_array\n",
    "    vectors = [\n",
    "        cube_definition_array[1] - cube_definition_array[0],\n",
    "        cube_definition_array[2] - cube_definition_array[0],\n",
    "        cube_definition_array[3] - cube_definition_array[0]\n",
    "    ]\n",
    "    points += [cube_definition_array[0] + vectors[0] + vectors[1]]\n",
    "    points += [cube_definition_array[0] + vectors[0] + vectors[2]]\n",
    "    points += [cube_definition_array[0] + vectors[1] + vectors[2]]\n",
    "    points += [cube_definition_array[0] + vectors[0] + vectors[1] + vectors[2]]\n",
    "    points = np.array(points)\n",
    "    edges = [\n",
    "        [points[0], points[3], points[5], points[1]],\n",
    "        [points[1], points[5], points[7], points[4]],\n",
    "        [points[4], points[2], points[6], points[7]],\n",
    "        [points[2], points[6], points[3], points[0]],\n",
    "        [points[0], points[2], points[4], points[1]],\n",
    "        [points[3], points[6], points[7], points[5]]\n",
    "    ]\n",
    "    faces = Poly3DCollection(edges, linewidths=1, edgecolors='k')\n",
    "    faces.set_facecolor((0,0,1,0.1))\n",
    "    faces.set_zorder(3)\n",
    "    ax.add_collection3d(faces)\n",
    "    # Plot the points themselves to force the scaling of the axes\n",
    "    ax.scatter(points[:,0], points[:,1], points[:,2], c=points[:,2], s=0)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.view_init(elev=orientation[0], azim=orientation[1], roll=orientation[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4a. Visualise tactile and visual data for 1 successful and 1 unsuccessful grasp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "successful_grasps = np.where(grasp_outcomes_data == 1)[0]\n",
    "unsuccessful_grasps = np.where(grasp_outcomes_data == 0)[0]\n",
    "\n",
    "# Randomly select one index from each set\n",
    "successful_rand_idx = np.random.choice(successful_grasps)\n",
    "unsuccessful_rand_idx = np.random.choice(unsuccessful_grasps)\n",
    "\n",
    "rand_indices = np.array([successful_rand_idx, unsuccessful_rand_idx])\n",
    "\n",
    "\n",
    "# Note that the plots use the \"..._data\" datasets instead of the \"..._ds\" datasets since \n",
    "# the \"..._ds\" datasets are already flattened for training\n",
    "for i in range(2):\n",
    "    fig = plt.figure(figsize=(14, 3))\n",
    "    # fig.suptitle(\"Successful grasp\" if grasp_outcomes_data[rand_indices[i]].item() == 1.0 else \"Unsuccessful grasp\")\n",
    "    \n",
    "    ax1 = fig.add_subplot(1, 3, 1)\n",
    "    ax1.set_xlabel(\"Depth\")\n",
    "    ax1.imshow(np.array(depth_data[rand_indices[i]]), cmap='gray')\n",
    "\n",
    "    ax2 = fig.add_subplot(1, 3, 2)\n",
    "    ax2.set_xlabel(\"Colour\")\n",
    "    ax2.imshow(np.array(color_data[rand_indices[i]]) / 255., cmap='gray')\n",
    "    \n",
    "    ax3 = fig.add_subplot(1, 3, 3, projection='3d')\n",
    "    ax3.set_xlabel('x position')\n",
    "    ax3.set_ylabel('y position')\n",
    "    ax3.set_zlabel('z position')\n",
    "    ax3.set_xlim3d(-0.05, 0.05)\n",
    "    ax3.set_ylim3d(-0.05, 0.05)\n",
    "    ax3.set_zlim3d(0, 0.1)\n",
    "\n",
    "    ax3.spines['top'].set_visible(False)\n",
    "    ax3.spines['right'].set_visible(False)\n",
    "    ax3.spines['bottom'].set_visible(False)\n",
    "    ax3.spines['left'].set_visible(False)\n",
    "\n",
    "    pose_data = poses_data[rand_indices]\n",
    "    plot_ee_pose(pose_data=poses_data[rand_indices[i]], grasp_outcome=grasp_outcomes_data[rand_indices[i]], ax=ax3, orientation=(30, 45, 0))\n",
    "    plot_3d_box(ax3, orientation=(30, 45, 0))\n",
    "\n",
    "fig.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4b. Visualise all end effector poses on box as skeleton hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 3 plots: 1 with the box only, remaining 2 with all poses each at different viewing orientations\n",
    "for i in range(3):\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    ax = fig.add_subplot(111, projection='3d', computed_zorder=False)\n",
    "    \n",
    "    # Plot unit vectors\n",
    "    x_unit = np.array([0.1, 0, 0])\n",
    "    y_unit = np.array([0, 0.075, 0])\n",
    "    z_unit = np.array([0, 0, 0.125])\n",
    "    ax.plot([0, x_unit[0]], [0, x_unit[1]], [0, x_unit[2]], color='r')\n",
    "    ax.plot([0, y_unit[0]], [0, y_unit[1]], [0, y_unit[2]], color='g')\n",
    "    ax.plot([0, z_unit[0]], [0, z_unit[1]], [0, z_unit[2]], color='b')\n",
    "\n",
    "    ax.text(x_unit[0], x_unit[1], x_unit[2], 'X')\n",
    "    ax.text(y_unit[0], y_unit[1], y_unit[2], 'Y')\n",
    "    ax.text(z_unit[0], z_unit[1], z_unit[2], 'Z')\n",
    "\n",
    "    # Remove all the axis and their labels\n",
    "    ax.axis('off')\n",
    "    ax.set_aspect('equal')\n",
    "    plot_3d_box(ax, orientation=(15, 30, 0))\n",
    "    if i == 0:\n",
    "        ax.set_xlim3d(-0.05, 0.05)\n",
    "        ax.set_ylim3d(-0.05, 0.05)\n",
    "        ax.set_zlim3d(0, 0.1)\n",
    "    if i > 0:\n",
    "        ax.set_xlim3d(-0.05, 0.05)\n",
    "        ax.set_ylim3d(-0.05, 0.05)\n",
    "        ax.set_zlim3d(0, 0.1)\n",
    "        for i in range(poses_data.shape[0]):\n",
    "            plot_ee_pose(pose_data=poses_data[i], grasp_outcome=grasp_outcomes_data[i], ax=ax, orientation=(15, 30+90*(i-1), 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train our Logistic Regression models on the 3 combinations of our data (tactile, visual, both):\n",
    "- Raw data\n",
    "- Principal Component Analysis - 2 main components\n",
    "- Convolutional Neural Network processed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tactile_train, X_tactile_test, y_tactile_train, y_tactile_test = train_test_split(tactile_ds.reshape(tactile_ds.shape[0], -1), grasp_outcomes_data, test_size=0.2, random_state=0)\n",
    "X_visual_train, X_visual_test, y_visual_train, y_visual_test = train_test_split(visual_ds, grasp_outcomes_data, test_size=0.2, random_state=0)\n",
    "X_complete_train, X_complete_test, y_complete_train, y_complete_test = train_test_split(complete_ds, grasp_outcomes_data, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.1 Raw data (tactile only) + LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_511 = LogisticRegression(random_state=0, max_iter=1000)\n",
    "model_511.fit(X_tactile_train, y_tactile_train)\n",
    "model_511_predictions = model_511.predict(X_tactile_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.2 Raw data (visual only) + LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_512 = LogisticRegression(random_state=0, max_iter=1000)\n",
    "model_512.fit(X_visual_train, y_visual_train)\n",
    "model_512_predictions = model_512.predict(X_visual_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.3 Raw data (both) + LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_513 = LogisticRegression(random_state=0, max_iter=1000)\n",
    "model_513.fit(X_complete_train, y_complete_train)\n",
    "model_513_predictions = model_513.predict(X_complete_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 CNN for dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple convolutional neural network that extracts features from an input tensor\n",
    "class FeatureExtractorCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractorCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 8, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(f\"Input size: {x.shape}\")\n",
    "        x = self.conv1(x)\n",
    "        # print(f\"Shape after conv1: {x.shape}\")\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool(x)\n",
    "        # print(f\"Shape after pool1: {x.shape}\")\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        # print(f\"Shape after conv2: {x.shape}\")\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool(x)\n",
    "        # print(f\"Shape after pool2: {x.shape}\")\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        # print(f\"Shape after conv3: {x.shape}\")\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool(x)\n",
    "        # print(f\"Shape after pool3: {x.shape}\")\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data using CNN feature extraction\n",
    "cnn = FeatureExtractorCNN()\n",
    "cnn_tactile = torch.cat([cnn(img.float().permute(2,0,1)).unsqueeze(0) for img in tactile_ds])\n",
    "cnn_tactile = cnn_tactile.reshape(cnn_tactile.shape[0], -1)\n",
    "cnn_tactile.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We simply combine the cnn-processed tactile data (from Section 5.3.1) with the visual data\n",
    "cnn_complete_ds = torch.cat([cnn_tactile.reshape(cnn_tactile.shape[0], -1), visual_ds], dim=1)\n",
    "cnn_complete_ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cnn_tactile_train, X_cnn_tactile_test, y_cnn_tactile_train, y_cnn_tactile_test = train_test_split(cnn_tactile.detach().numpy(), grasp_outcomes_data, test_size=0.2, random_state=0)\n",
    "X_cnn_complete_train, X_cnn_complete_test, y_cnn_complete_train, y_cnn_complete_test = train_test_split(cnn_complete_ds.detach().numpy(), grasp_outcomes_data, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.1 CNN (tactile only) + LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_531 = LogisticRegression(random_state=0, max_iter=1000)\n",
    "model_531.fit(X_cnn_tactile_train, y_cnn_tactile_train)\n",
    "model_531_predictions = model_531.predict(X_cnn_tactile_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.2 CNN (visual only) + LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.3 CNN (both) + LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_533 = LogisticRegression(random_state=0, max_iter=1000)\n",
    "model_533.fit(X_cnn_complete_train, y_cnn_complete_train)\n",
    "model_533_predictions = model_533.predict(X_cnn_complete_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.4 CNN (tactile only) + PCA + LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(tensor, k, verbose=False):\n",
    "    if len(tensor) == 3:\n",
    "        tensor = tensor.reshape((tensor.shape[0], -1))\n",
    "    \n",
    "    pca = PCA(n_components=k)\n",
    "    pca.fit(tensor)\n",
    "\n",
    "    variance = sum(pca.explained_variance_ratio_) * 100\n",
    "    # print(f\"Variance captured for {k} components: {variance:.2f}%\")\n",
    "\n",
    "    if verbose:\n",
    "        return pca.transform(tensor), k, variance\n",
    "    else:\n",
    "        return pca.transform(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select $k$ principal components that correspond to 90% variance of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 0\n",
    "\n",
    "k_variances = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 30):\n",
    "    pca_cnn_tactile_ds, k, variance = pca(cnn_tactile.detach().numpy(), k=i, verbose=True)\n",
    "    k_variances.append(variance)\n",
    "    if variance > 90:\n",
    "        print(f\"Choose {k} components for CNN + tactile only dataset.\")\n",
    "        K = k\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(0, K, 5):\n",
    "    if k > 0:\n",
    "        print(f\"Variance for {k} components: {k_variances[k]:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot number of components against variance captured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(k_variances)\n",
    "plt.xlabel('Number of components ' + r'$k$')\n",
    "plt.ylabel('Variance (%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_cnn_tactile_ds = pca(cnn_tactile.detach().numpy(), k=5)\n",
    "X_pca_cnn_tactile_train, X_pca_cnn_tactile_test, y_pca_cnn_tactile_train, y_pca_cnn_tactile_test = train_test_split(pca_cnn_tactile_ds, grasp_outcomes_data, test_size=0.2, random_state=0)\n",
    "model_534 = LogisticRegression(random_state=0, max_iter=1000)\n",
    "model_534.fit(X_pca_cnn_tactile_train, y_pca_cnn_tactile_train)\n",
    "model_534_predictions = model_534.predict(X_pca_cnn_tactile_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_cnn_complete_ds = pca(cnn_complete_ds.detach().numpy(), k=5)\n",
    "X_pca_cnn_complete_train, X_pca_cnn_complete_test, y_pca_cnn_complete_train, y_pca_cnn_complete_test = train_test_split(pca_cnn_complete_ds, grasp_outcomes_data, test_size=0.2, random_state=0)\n",
    "model_535 = LogisticRegression(random_state=0, max_iter=1000)\n",
    "model_535.fit(X_pca_cnn_complete_train, y_pca_cnn_complete_train)\n",
    "model_535_predictions = model_535.predict(X_pca_cnn_complete_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrices(model_data, fig_height):\n",
    "    fig, axn = plt.subplots(1, len(model_data), sharex=True, sharey=True, figsize=(12, fig_height))\n",
    "\n",
    "    for i, ax in enumerate(axn.flat):\n",
    "        try:\n",
    "            model_name = list(model_data.keys())[i]\n",
    "            model = model_data[model_name]\n",
    "            preds = model[\"preds\"]\n",
    "            score = model[\"score\"]\n",
    "            test_set = model[\"test_set\"]\n",
    "\n",
    "            if score is not None and preds is not None and test_set is not None:\n",
    "                cm = confusion_matrix(test_set, preds)\n",
    "                sns.heatmap(cm, linewidths=1, ax=ax, annot=True, fmt='g')\n",
    "                ax.set_title(model_name + f\": {score*100:.2f}%\", fontsize=8)\n",
    "            else:\n",
    "                ax.set_title(model_name + \"No results\", fontsize=8)\n",
    "            continue\n",
    "        except IndexError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = {\n",
    "    \"Raw (tactile only)\": {\"score\": model_511.score(X_tactile_test, y_tactile_test), \"preds\": model_511_predictions, \"test_set\": y_tactile_test},\n",
    "    \"Raw (visual only)\": {\"score\": model_512.score(X_visual_test, y_visual_test), \"preds\": model_512_predictions, \"test_set\": y_visual_test},\n",
    "    \"Raw (both)\": {\"score\": model_513.score(X_complete_test, y_complete_test),\"preds\": model_513_predictions,\"test_set\": y_complete_test},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrices(model_data, fig_height=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = {\n",
    "    \"CNN (tactile only)\": {\"score\": model_531.score(X_cnn_tactile_test, y_cnn_tactile_test), \"preds\": model_531_predictions,\"test_set\": y_cnn_tactile_test},\n",
    "    \"CNN (both)\": {\"score\": model_533.score(X_cnn_complete_test, y_cnn_complete_test), \"preds\": model_533_predictions, \"test_set\": y_cnn_complete_test},\n",
    "    \"CNN (tactile only) + PCA\": {\"score\": model_534.score(X_pca_cnn_tactile_test, y_pca_cnn_tactile_test), \"preds\": model_534_predictions, \"test_set\": y_pca_cnn_tactile_test},\n",
    "    \"CNN (both) + PCA\": {\"score\": model_535.score(X_pca_cnn_complete_test, y_pca_cnn_complete_test), \"preds\": model_535_predictions, \"test_set\": y_pca_cnn_complete_test}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrices(model_data, fig_height=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ee1153b3f7d4e5dcd3632df88b5826f13fc1401ea1edd930ba216b62830b02e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
